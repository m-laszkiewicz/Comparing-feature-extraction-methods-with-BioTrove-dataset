{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0ca6169-d5e3-4499-9ac6-ec7e48468508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import statements\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "278c283c-e6ca-43f6-8272-410349fa91da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata preview:\n",
      "                    hash_id       family\n",
      "0  223m6ywujk3htx2s3kfqx7ee  Acanthaceae\n",
      "1  2aba7w224g4tso44mtzpnizg  Acanthaceae\n",
      "2  2dovrj4uex7apou4zyu7nau7  Acanthaceae\n",
      "3  2f53p6wsfhsnik2sy3jxn2ok  Acanthaceae\n",
      "4  2fvqsa7ldatavhuevcvia5lm  Acanthaceae\n",
      "Indexed 49,633 image files\n",
      "Updated df_meta:\n",
      "                    hash_id       family               hash_id_str  \\\n",
      "0  223m6ywujk3htx2s3kfqx7ee  Acanthaceae  223m6ywujk3htx2s3kfqx7ee   \n",
      "1  2aba7w224g4tso44mtzpnizg  Acanthaceae  2aba7w224g4tso44mtzpnizg   \n",
      "2  2dovrj4uex7apou4zyu7nau7  Acanthaceae  2dovrj4uex7apou4zyu7nau7   \n",
      "3  2f53p6wsfhsnik2sy3jxn2ok  Acanthaceae  2f53p6wsfhsnik2sy3jxn2ok   \n",
      "4  2fvqsa7ldatavhuevcvia5lm  Acanthaceae  2fvqsa7ldatavhuevcvia5lm   \n",
      "\n",
      "                                                path  \n",
      "0  C:\\Users\\mlasz\\OneDrive\\Desktop\\mlm_25\\biotrov...  \n",
      "1  C:\\Users\\mlasz\\OneDrive\\Desktop\\mlm_25\\biotrov...  \n",
      "2  C:\\Users\\mlasz\\OneDrive\\Desktop\\mlm_25\\biotrov...  \n",
      "3  C:\\Users\\mlasz\\OneDrive\\Desktop\\mlm_25\\biotrov...  \n",
      "4  C:\\Users\\mlasz\\OneDrive\\Desktop\\mlm_25\\biotrov...  \n",
      "\n",
      "Found 49,633 matches; 0 missing.\n"
     ]
    }
   ],
   "source": [
    "# Paths to image and metadata\n",
    "# NOTE: Add your file paths for images AND csv below\n",
    "img_dir = \"[YOUR IMAGE DIRECTORY PATH GOES HERE]\"    \n",
    "csv_path = \"[YOUR METADATA CSV PATH GOES HERE]\"        \n",
    "\n",
    "# Load metadata + sanity check assertions\n",
    "df_meta = pd.read_csv(csv_path)\n",
    "assert 'hash_id' in df_meta.columns, \"CSV missing 'hash_id' column\"\n",
    "assert 'family' in df_meta.columns, \"CSV missing 'family' column\"\n",
    "\n",
    "# Inspect metadata\n",
    "print(\"Metadata preview:\")\n",
    "print(df_meta.head())\n",
    "\n",
    "# Map images to csv filepaths\n",
    "image_dir = Path(img_dir)\n",
    "allowed_exts = {'.jpg', '.jpeg', '.png'}\n",
    "\n",
    "# Index all images once (recursive, meaning: look into all directories and subdirectories)\n",
    "all_paths = [p for p in image_dir.rglob(\"*\") if p.suffix.lower() in allowed_exts]\n",
    "print(f\"Indexed {len(all_paths):,} image files\")\n",
    "\n",
    "# Build stem to list(paths) mapping\n",
    "# Defaultdict(list) creates an empty list for new keys and '.append' adds the object 'p' as a string in that list for every path in our list 'all_paths'\n",
    "stem_to_paths = defaultdict(list)\n",
    "for p in all_paths:\n",
    "    stem_to_paths[p.stem].append(str(p)) # p.stem is filepath without the extension (without jpeg, jpg, or png)\n",
    "\n",
    "# Create a simple first-match dict (stem to first path)\n",
    "# If images share a hashid, the first image \"found\" and connected to a hashid is kept\n",
    "first_match = {stem: paths[0] for stem, paths in stem_to_paths.items()}\n",
    "\n",
    "# Ensure hash_id is a string and has no whitespace by creating 'hash_id_str' column and adding to df_meta\n",
    "df_meta['hash_id_str'] = df_meta['hash_id'].astype(str).str.strip()\n",
    "# Create column for path for each hash_id and add to df_meta\n",
    "df_meta['path'] = df_meta['hash_id_str'].map(first_match)  # NaN for missing\n",
    "# show df_meta\n",
    "print(\"Updated df_meta:\")\n",
    "print(df_meta.head())\n",
    "\n",
    "# Create objects for: found metdata successfully linked image paths to hashids & object for unsucessfuly linked hashids\n",
    "df_meta_found = df_meta[df_meta['path'].notna()].reset_index(drop=True)\n",
    "hashid_to_path = dict(zip(df_meta_found['hash_id_str'], df_meta_found['path']))\n",
    "missing_hashids = df_meta[df_meta['path'].isna()]['hash_id_str'].tolist()\n",
    "\n",
    "# Print numbers of found and missing \n",
    "print(f\"\\nFound {len(hashid_to_path):,} matches; {len(missing_hashids):,} missing.\")\n",
    "if missing_hashids:\n",
    "    print(\"Example missing ids:\", missing_hashids[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03ce3fd6-ed7d-4782-bbd1-90bf79a7c1a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and dataloader\n",
    "\n",
    "# Image transforms - catered to ImageNet input data since ResNet models use pretrained ImageNet weights\n",
    "img_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Dataset\n",
    "class ClusteringBioTroveDataset(Dataset):\n",
    "    def __init__(self, mapping, transform=None):\n",
    "        self.items = list(mapping.items())\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.items)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        hid, path = self.items[idx]\n",
    "        with Image.open(path) as img:\n",
    "            img = img.convert('RGB')\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "        return hid, img\n",
    "\n",
    "dataset = ClusteringBioTroveDataset(hashid_to_path, img_transform)\n",
    "# Adjust batch_size and num_workers as needed\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False, num_workers=0, pin_memory=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlm_25(biotrove)",
   "language": "python",
   "name": "biotrove"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
