# Reflection

This research project reflects a semester-long effort on the “Clustering-BioTrove” challenge with the BioTrove_1 team. The idea behind it occurred rather naturally as I was tirelessly trying to find some way to boost the team score by even the smallest amount. Although the feature extraction method ultimately didn’t increase our team score by too much, in the context of unsupervised clustering problems in the field of environmental data science, it can impact results by creating more easily-separable data. As such, the exploration and visual comparison of feature extraction methods was not only a very informative experience for me, it also felt practical to my future work in machine learning in the context of environmental data.
Comparing the following four feature extraction methods: single-layer ResNet50, double-layer-ResNet50, double-layer ResNet101, and double-layer ResNet50 with supervised contrastive learning, was a generally successful experiment. I feel that the results both reflected some of the expectations I had, but also provided new insights. For example, I expected the ResNet101 model to outperform the ResNet50 model by creating more separable data. However, I did not consider what the dimensionally-reduced and visualized embeddings would look like and how that might further my understanding of ResNet models in general. Being able to visually compare embeddings from both models helped me understand that although subtle, deeper ResNet models can result in improved results. I think the visualization approach used in this experiment, UMAP for dimensionality reduction and then plotting using matplotlib and seaborn libraries in Python, was particularly successful at comparing and conveying results. With UMAP parameters kept constant, the resulting plots showed the variations in extracted embeddings across the different feature extraction methods in a clear, easy to understand way. This felt like a success of the project. I was pleased that months of work on the different feature extraction methods culminated in such a simple but effective comparison.
As with any good research project, I am left with new doubts and curiosities. The subjective nature of the metric used to evaluate the embedding plots: visual inspection, seems like a weakness of the experiment. Although it provides insight into how each feature extraction method is performing, I wonder if a more objective evaluation metric, in addition to the embedding plots, would give more validity to the experiment. To address this, an interesting future approach would be to retrieve the ground truth genus and species labels from the original BioTrove dataset, and use a constant (through parameters) k-means clustering approach on embeddings from each feature extraction method. In this way, metrics such as precision, recall, and F1 score could be used to compare and evaluate the quality of embeddings in a more objective manner.   

